{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Vjx7LfOJDP3YdER8Blir_DYvvfJHg-0n","timestamp":1709812545489},{"file_id":"1AAczjwTTAtf2LCLtQR49seR_hKbD6QtY","timestamp":1709808756153},{"file_id":"15QkHKRpw6BQb_1LyhPbRRfD2ES4z1Tws","timestamp":1709808731962},{"file_id":"1y2EvYXdh7L6heGRg1eitKYmbSeJ5MaWt","timestamp":1709768055112},{"file_id":"1IoU5EfFy7xYgwNmCrWFZEnu_7O6WZaWc","timestamp":1709768018094},{"file_id":"13qTnZ_gq-Q6uEa53iRPz_5TcXratPcKq","timestamp":1709727562896},{"file_id":"1Myg7taWSoN0OTqOoL2mwi4P-6Ue2aj3A","timestamp":1709727542048},{"file_id":"15kJ84UYrv6O3oOY47_phbF2i8xszO5ID","timestamp":1709717350062},{"file_id":"1GlGD2epB1QGwIvJHfUt_nx5B5KHOMFyg","timestamp":1709708904074},{"file_id":"1PSaEKQF6Y-L4BolE-5BRmhFAWuZN9pCP","timestamp":1709704822562},{"file_id":"1xE4PpWcK-0Px-piG_jmx5z3JJwhru1yg","timestamp":1709704811824},{"file_id":"1JYDKj4YfBVqh0eBcY92-fvY6jhzZDkiS","timestamp":1708958579028},{"file_id":"1PaCGvtQwEuTbGHiyt7DPf86DHPnhdJ-K","timestamp":1708791879106},{"file_id":"1wEJVLeUEIOG3cBTTbpnwEufnouzhhQmG","timestamp":1708773231452},{"file_id":"11w3xUhgKC2q_6GZ6hYM8wjqJ0LeF4tOC","timestamp":1708773191999},{"file_id":"1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx","timestamp":1708759455015}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Targets: Above 99% test_acc with very less number of params  \n","Results:  \n","    best test_acc = 99.1% on 12th epoch  \n","\n","    total parameters = 6016  \n","Analysis:  It has Batch normalization, Regularization,  slightly increased capacity and removed GAP\n","File Link:  "],"metadata":{"id":"TdwmELLoDslu"}},{"cell_type":"markdown","source":["# 1. Import external libraries"],"metadata":{"id":"iRTufnR1yQuf"}},{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT","executionInfo":{"status":"ok","timestamp":1710506209260,"user_tz":-330,"elapsed":11703,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}}},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["#### Basically working model\n","#### Img Aug RandomRotation\n","#### StepLR Scheduler\n","#### Train & Test Graphs"],"metadata":{"id":"Yr67RCeG8Xc7"}},{"cell_type":"markdown","source":["# 2. Convolutional Neural Network (model) architecture"],"metadata":{"id":"lrJhILplyZZA"}},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7","executionInfo":{"status":"ok","timestamp":1710506209263,"user_tz":-330,"elapsed":50,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        set1 = 8 #channels\n","        set2 = 8 #channels\n","        out = 10 #channels\n","        avg = 7 #channels\n","        self.conv1 = nn.Conv2d(1, set1, 3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_features=set1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.conv2 = nn.Conv2d(set1, set1, 3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_features=set1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv2d(set1, set2, 3, padding=1)\n","        self.bn3 = nn.BatchNorm2d(num_features=set2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.conv4 = nn.Conv2d(set2, set2, 3, padding=1)\n","        self.bn4 = nn.BatchNorm2d(num_features=set2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","        self.conv5 = nn.Conv2d(set2, set2, 3, padding=1)\n","        self.bn5 = nn.BatchNorm2d(num_features=set2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.conv6 = nn.Conv2d(set2, out, 3, padding=1)\n","        self.bn6 = nn.BatchNorm2d(num_features=out, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","        self.conv7 = nn.Conv2d(out, out, 3, padding=1)\n","        self.bn7 = nn.BatchNorm2d(num_features=out, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.conv8 = nn.Conv2d(out, out, 3, padding=1)\n","        self.bn8 = nn.BatchNorm2d(num_features=out, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        self.conv9 = nn.Conv2d(out, out, 3)\n","        self.bn9 = nn.BatchNorm2d(num_features=out, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","\n","\n","        self.drop = nn.Dropout(0.25)\n","        self.gap = nn.AvgPool2d(kernel_size=[avg,avg], stride=[avg,avg], padding=0, ceil_mode=False, count_include_pad=False)\n","\n","    def forward(self, x):\n","        x = self.drop(self.pool1(self.bn2(F.relu(self.conv2(self.bn1(F.relu(self.conv1(x))))))))\n","        x = self.drop(self.pool2(self.bn4(F.relu(self.conv4(self.bn3(F.relu(self.conv3(x))))))))\n","        x = self.drop(self.pool3(self.bn6(F.relu(self.conv6(self.bn5(F.relu(self.conv5(x)))))))) # ToDo Try adding MP here\n","        x = self.drop(self.bn8(F.relu(self.conv8(self.bn7(F.relu(self.conv7(x))))))) # ToDo Try adding MP here\n","        x = self.conv9(x)\n","        #x = self.gap(x) # Raja ToDo Try printing shape here\n","        #print(x.shape)\n","        x = x.view(-1, 10) # Raja ToDo Try printing shape here\n","        return F.log_softmax(x)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### RuntimeError: running_mean should contain 8 elements not 4\n","Implies check the bn channels, whether it matches with the ouput channel of its previous layer."],"metadata":{"id":"C9C_o3nrrqVl"}},{"cell_type":"markdown","source":["# 3. Display summary of model"],"metadata":{"id":"4aE6iP1Dyl1l"}},{"cell_type":"code","metadata":{"id":"xdydjYTZFyi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710506210497,"user_tz":-330,"elapsed":1280,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}},"outputId":"2c1073ea-ddc3-4aa2-dfd3-a7b6b1938c88"},"source":["#!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available() #bool\n","str_gpu_cpu = \"cuda\" if use_cuda else \"cpu\" #string\n","device = torch.device(str_gpu_cpu)\n","print(\"device is \" + str_gpu_cpu)\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["device is cuda\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","            Conv2d-3            [-1, 8, 28, 28]             584\n","       BatchNorm2d-4            [-1, 8, 28, 28]              16\n","         MaxPool2d-5            [-1, 8, 14, 14]               0\n","           Dropout-6            [-1, 8, 14, 14]               0\n","            Conv2d-7            [-1, 8, 14, 14]             584\n","       BatchNorm2d-8            [-1, 8, 14, 14]              16\n","            Conv2d-9            [-1, 8, 14, 14]             584\n","      BatchNorm2d-10            [-1, 8, 14, 14]              16\n","        MaxPool2d-11              [-1, 8, 7, 7]               0\n","          Dropout-12              [-1, 8, 7, 7]               0\n","           Conv2d-13              [-1, 8, 7, 7]             584\n","      BatchNorm2d-14              [-1, 8, 7, 7]              16\n","           Conv2d-15             [-1, 10, 7, 7]             730\n","      BatchNorm2d-16             [-1, 10, 7, 7]              20\n","        MaxPool2d-17             [-1, 10, 3, 3]               0\n","          Dropout-18             [-1, 10, 3, 3]               0\n","           Conv2d-19             [-1, 10, 3, 3]             910\n","      BatchNorm2d-20             [-1, 10, 3, 3]              20\n","           Conv2d-21             [-1, 10, 3, 3]             910\n","      BatchNorm2d-22             [-1, 10, 3, 3]              20\n","          Dropout-23             [-1, 10, 3, 3]               0\n","           Conv2d-24             [-1, 10, 1, 1]             910\n","================================================================\n","Total params: 6,016\n","Trainable params: 6,016\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.29\n","Params size (MB): 0.02\n","Estimated Total Size (MB): 0.31\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-fc10b8eba6d6>:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n"]}]},{"cell_type":"markdown","source":["# 4. Preparation of dataset  "],"metadata":{"id":"aTtGaP1BytH_"}},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710506212059,"user_tz":-330,"elapsed":1569,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}},"outputId":"bd18f6ba-93d0-4322-bfa8-a63ec82d4961"},"source":["\n","SEED = 1\n","torch.manual_seed(SEED)\n","batch_size = 128\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 92919906.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 61930313.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 32444602.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 19223540.63it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# 5. Train and Test Functions"],"metadata":{"id":"Sy-lDNgGyzfR"}},{"cell_type":"code","metadata":{"id":"8fDefDhaFlwH","executionInfo":{"status":"ok","timestamp":1710506212060,"user_tz":-330,"elapsed":12,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}}},"source":["from tqdm import tqdm\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","    correct = 0\n","    processed = 0\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        train_losses.append(loss)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(desc= f'epoch={epoch} loss={loss.item()} batch_id={batch_idx}')\n","        pred = output.argmax(dim=1, keepdim=True)\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","        train_acc.append(100*correct/processed)\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()\n","            test_losses.append(test_loss)\n","            pred = output.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\n  Test set: Average loss: {:.4f}, Test Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    test_acc.append(100. * correct / len(test_loader.dataset))\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 6. Run the model with a device and an optimizer"],"metadata":{"id":"OKuLqu0Jy-pW"}},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"32158f7f-a117-4d28-b35e-a46d88c073fb","executionInfo":{"status":"ok","timestamp":1710506628228,"user_tz":-330,"elapsed":416178,"user":{"displayName":"raja p nitc","userId":"00071711281456970631"}}},"source":["\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","ep = 21\n","for epoch in range(1, ep):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)\n","\n","#With params = 2158, on 7th epoch, test accuracy = 94.15%\n","#With params = 4018, on 8th epoch, test accuracy = 95.9% with imgaug\n","#With params = 4906, on 8th epoch, test accuracy = 98.2% without imgaug\n","#With params = 6016, on 8th epoch, test accuracy = 99.02% without imgaug"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-2-fc10b8eba6d6>:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)\n","epoch=1 loss=0.074665367603302 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0757, Test Accuracy: 9756/10000 (97.56%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=2 loss=0.11504825204610825 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0584, Test Accuracy: 9807/10000 (98.07%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=3 loss=0.09645208716392517 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0500, Test Accuracy: 9836/10000 (98.36%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=4 loss=0.17671720683574677 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0411, Test Accuracy: 9868/10000 (98.68%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=5 loss=0.05189940705895424 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0385, Test Accuracy: 9874/10000 (98.74%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=6 loss=0.0948757603764534 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0413, Test Accuracy: 9871/10000 (98.71%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=7 loss=0.054727163165807724 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0360, Test Accuracy: 9883/10000 (98.83%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=8 loss=0.05284030735492706 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0372, Test Accuracy: 9888/10000 (98.88%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=9 loss=0.15050342679023743 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 24.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0324, Test Accuracy: 9899/10000 (98.99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=10 loss=0.040694840252399445 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0365, Test Accuracy: 9890/10000 (98.90%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=11 loss=0.04004470631480217 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0324, Test Accuracy: 9899/10000 (98.99%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=12 loss=0.03488033264875412 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0279, Test Accuracy: 9910/10000 (99.10%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=13 loss=0.1315692961215973 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0288, Test Accuracy: 9911/10000 (99.11%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=14 loss=0.042361900210380554 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0295, Test Accuracy: 9903/10000 (99.03%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=15 loss=0.026655392721295357 batch_id=468: 100%|██████████| 469/469 [00:20<00:00, 22.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0285, Test Accuracy: 9907/10000 (99.07%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=16 loss=0.10933852940797806 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0276, Test Accuracy: 9914/10000 (99.14%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=17 loss=0.0691598579287529 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0334, Test Accuracy: 9888/10000 (98.88%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=18 loss=0.08772187680006027 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0294, Test Accuracy: 9909/10000 (99.09%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=19 loss=0.0071752858348190784 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 24.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0273, Test Accuracy: 9914/10000 (99.14%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["epoch=20 loss=0.010388033464550972 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Test set: Average loss: 0.0269, Test Accuracy: 9909/10000 (99.09%)\n","\n"]}]},{"cell_type":"markdown","source":["Raja ToDo :\n","Try below ::\n","1. BatchNormalization\n","2. Dropout\n","3. LR scheduler\n","4. GAP"],"metadata":{"id":"5CiZFW5WLMnP"}}]}